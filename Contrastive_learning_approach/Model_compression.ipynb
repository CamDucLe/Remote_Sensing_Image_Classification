{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install packages + check hardware"
      ],
      "metadata": {
        "id": "4cfOTUa9uNsm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6XAcSTDuHPR"
      },
      "outputs": [],
      "source": [
        "! pip install tensorflow==2.9.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "restart runtime"
      ],
      "metadata": {
        "id": "HWa65h5ouRkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_model_optimization"
      ],
      "metadata": {
        "id": "7SHp_JzquTfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "print(\"tensorflow version: \", tf.__version__) # 2.9.0\n",
        "!nvidia-smi -L"
      ],
      "metadata": {
        "id": "M0m2zQKfuWxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Post-training quantization"
      ],
      "metadata": {
        "id": "qV2Z7oHtuXdQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantization.py"
      ],
      "metadata": {
        "id": "6xycbywXue-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class Contrastive_Loss_2(tf.keras.losses.Loss):\n",
        "  def __init__(self, temperature=0.5, rate=0.5, name='Contrastive_Loss_2', **kwargs):\n",
        "    super(Contrastive_Loss_2, self).__init__(name=name, **kwargs)\n",
        "    self.temperature   = temperature\n",
        "    self.rate          = rate\n",
        "    self.cosine_sim    = tf.keras.losses.CosineSimilarity(axis=-1, reduction=tf.keras.losses.Reduction.NONE)\n",
        "    \n",
        "  # @tf.function\n",
        "  def call(self, z1, z2):\n",
        "    batch_size, n_dim = z1.shape\n",
        "\n",
        "    # Compute Euclid Distance loss\n",
        "    difference    = z1 - z2                                             # (BxB)   * z1 and z2 already applied soft max -> in the last axis, max dif will be 1 \n",
        "    squared_norm  = tf.reduce_sum(tf.square(difference), axis=1)        # (B)\n",
        "    distance      = tf.sqrt(squared_norm + 1e-8)                        # (B)     * + epsilon to avoid Nan in gradient\n",
        "    mean_distance = tf.reduce_mean(distance)                            # () -> scalar\n",
        "    tf.debugging.check_numerics(mean_distance.numpy(), 'Distance contains NaN values.')\n",
        "    # print('distance: , ',mean_distance)\n",
        "\n",
        "    # Compute Consine Similarity loss\n",
        "    z = tf.concat((z1, z2), 0)\n",
        "\n",
        "    sim_ij      = - self.cosine_sim(z[:batch_size], z[batch_size:])     # (B)  -> batch_size pair\n",
        "    sim_ji      = - self.cosine_sim(z[batch_size:], z[:batch_size])     # (B)  -> batch_size pair\n",
        "    sim_pos     = tf.concat((sim_ij,sim_ji), axis=0)                    # (2B) -> 2*batch_size positive pair\n",
        "    numerator   = tf.math.exp(sim_pos / self.temperature)               # (2B) -> 2*batch_size positive pair\n",
        "  \n",
        "    sim_neg     = - self.cosine_sim(tf.expand_dims(z, 1), z)            # sim (Bx1xE, BxE) -> (2Bx2B)\n",
        "    mask        = 1 - tf.eye(2*batch_size, dtype=tf.float32)            # (2Bx2B)\n",
        "    sim_neg     = mask * tf.math.exp(sim_neg / self.temperature)        # (2Bx2B)\n",
        "    denominator = tf.math.reduce_sum(sim_neg, axis=-1)                  # (2B) \n",
        "  \n",
        "    mean_cosine_similarity = tf.reduce_mean(- tf.math.log((numerator + 1e-11) / (denominator + 1e-11)))       # () -> scalar\n",
        "    tf.debugging.check_numerics(mean_cosine_similarity.numpy(), 'Cosine contains NaN values.')\n",
        "    # print('similarity: , ',mean_cosine_similarity)\n",
        "\n",
        "    # Compute total loss with associated rate\n",
        "    total_loss = (1-self.rate)*mean_distance + self.rate*mean_cosine_similarity \n",
        "    tf.debugging.check_numerics(total_loss.numpy(), 'Total contains NaN values.')\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "7igcPdFPubwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "best_model = tf.keras.models.load_model( filepath='/content/drive/MyDrive/RSIC/NWPU-RESISC45/effb0_567_con_2loss_2/contrastive_model.h5', \n",
        "                                      custom_objects={'Contrastive_Loss_2': Contrastive_Loss_2})\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(best_model) # quantize to 8 bit\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT] # int 8\n",
        "tflite_quant_model = converter.convert()\n",
        "\n",
        "# #save converted quantization model to tflite format\n",
        "open(\"/content/drive/MyDrive/RSIC/NWPU-RESISC45/effb0_567_con_2loss_2/tflite_model.tflite\", \"wb\").write(tflite_quant_model)\n",
        "\n",
        "\n",
        "best_model_path = '/content/drive/MyDrive/RSIC/NWPU-RESISC45/effb0_567_con_2loss_2/contrastive_model.h5'\n",
        "quant_model_path = '/content/drive/MyDrive/RSIC/NWPU-RESISC45/effb0_567_con_2loss_2/tflite_model.tflite'\n",
        "\n",
        "print( os.path.getsize(best_model_path) / float(2**20))\n",
        "print( os.path.getsize(quant_model_path) / float(2**20))"
      ],
      "metadata": {
        "id": "_yY24-bvujaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate_tflite_model.py"
      ],
      "metadata": {
        "id": "vE2ca6qQurS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if physical_devices: # Use GPU for inference\n",
        "    tf.config.set_visible_devices(physical_devices[0], 'GPU')\n",
        "\n",
        "# tf.config.set_visible_devices([], 'GPU') # Use CPU for inference\n",
        "\n",
        "def Accuracy(y_true, y_pred):\n",
        "  y_true   = tf.argmax(y_true, axis=-1) # (B) \n",
        "  y_pred   = tf.argmax(y_pred, axis=-1) # (B)\n",
        "  correct  = tf.cast(y_true == y_pred, tf.float32)\n",
        "  accuracy = tf.reduce_sum(correct)\n",
        "  if y_true.shape != y_pred.shape:\n",
        "    raise ValueError('Something error in Acc calculation')\n",
        "  return accuracy.numpy()\n",
        "\n",
        "TEST_DIR        = '/content/dataset2/test/'\n",
        "test_generator  = TestGenerator(img_dir=TEST_DIR)\n",
        "quant_model_path = '/content/drive/MyDrive/RSIC/NWPU-RESISC45/effb0_567_con_2loss_2/tflite_model.tflite'\n",
        "interpreter = tf.lite.Interpreter(model_path=quant_model_path) # Load the TFLite model.\n",
        "interpreter.resize_tensor_input(0, [45, 256, 256, 3])\n",
        "interpreter.allocate_tensors() # Allocate memory for input and output tensors.\n",
        "\n",
        "start_test   = datetime.now()\n",
        "test_acc     = 0\n",
        "for n_batch_test in range(560):\n",
        "  x_test, y_true, n_imgs = test_generator.get_batch(n_batch_test)\n",
        "  input_details = interpreter.get_input_details()\n",
        "  interpreter.set_tensor(input_details[0]['index'], x_test)\n",
        "  interpreter.invoke()\n",
        "  output_details = interpreter.get_output_details()\n",
        "  y_pred         = interpreter.get_tensor(output_details[0]['index'])\n",
        "  test_acc      += Accuracy(y_true, y_pred)\n",
        "  \n",
        "test_acc /= 25200\n",
        "print('# test accuray: ', test_acc, '  and time needed for test: ', datetime.now()-start_test)"
      ],
      "metadata": {
        "id": "ChF_Pdo4utiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "img = Image.fromarray((x_test[-1].numpy()).astype(np.uint8), 'RGB') # *255\n",
        "img.show()"
      ],
      "metadata": {
        "id": "ia-TKtkEuwPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network pruning"
      ],
      "metadata": {
        "id": "xE6bbtSAvDTu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load_pre_trained_model.py"
      ],
      "metadata": {
        "id": "l88HWoUjvJCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "class Contrastive_Loss_2(tf.keras.losses.Loss):\n",
        "  def __init__(self, temperature=0.5, rate=0.5, name='Contrastive_Loss_2', **kwargs):\n",
        "    super(Contrastive_Loss_2, self).__init__(name=name, **kwargs)\n",
        "    self.temperature   = temperature\n",
        "    self.rate          = rate\n",
        "    self.cosine_sim    = tf.keras.losses.CosineSimilarity(axis=-1, reduction=tf.keras.losses.Reduction.NONE)\n",
        "    \n",
        "  # @tf.function\n",
        "  def call(self, z1, z2):\n",
        "    batch_size, n_dim = z1.shape\n",
        "\n",
        "    # Compute Euclid Distance loss\n",
        "    difference    = z1 - z2                                             # (BxB)   * z1 and z2 already applied soft max -> in the last axis, max dif will be 1 \n",
        "    squared_norm  = tf.reduce_sum(tf.square(difference), axis=1)        # (B)\n",
        "    distance      = tf.sqrt(squared_norm + 1e-8)                        # (B)     * + epsilon to avoid Nan in gradient\n",
        "    mean_distance = tf.reduce_mean(distance)                            # () -> scalar\n",
        "    tf.debugging.check_numerics(mean_distance.numpy(), 'Distance contains NaN values.')\n",
        "    # print('distance: , ',mean_distance)\n",
        "\n",
        "    # Compute Consine Similarity loss\n",
        "    z = tf.concat((z1, z2), 0)\n",
        "\n",
        "    sim_ij      = - self.cosine_sim(z[:batch_size], z[batch_size:])     # (B)  -> batch_size pair\n",
        "    sim_ji      = - self.cosine_sim(z[batch_size:], z[:batch_size])     # (B)  -> batch_size pair\n",
        "    sim_pos     = tf.concat((sim_ij,sim_ji), axis=0)                    # (2B) -> 2*batch_size positive pair\n",
        "    numerator   = tf.math.exp(sim_pos / self.temperature)               # (2B) -> 2*batch_size positive pair\n",
        "  \n",
        "    sim_neg     = - self.cosine_sim(tf.expand_dims(z, 1), z)            # sim (Bx1xE, BxE) -> (2Bx2B)\n",
        "    mask        = 1 - tf.eye(2*batch_size, dtype=tf.float32)            # (2Bx2B)\n",
        "    sim_neg     = mask * tf.math.exp(sim_neg / self.temperature)        # (2Bx2B)\n",
        "    denominator = tf.math.reduce_sum(sim_neg, axis=-1)                  # (2B) \n",
        "  \n",
        "    mean_cosine_similarity = tf.reduce_mean(- tf.math.log((numerator + 1e-11) / (denominator + 1e-11)))       # () -> scalar\n",
        "    tf.debugging.check_numerics(mean_cosine_similarity.numpy(), 'Cosine contains NaN values.')\n",
        "    # print('similarity: , ',mean_cosine_similarity)\n",
        "\n",
        "    # Compute total loss with associated rate\n",
        "    total_loss = (1-self.rate)*mean_distance + self.rate*mean_cosine_similarity \n",
        "    tf.debugging.check_numerics(total_loss.numpy(), 'Total contains NaN values.')\n",
        "    return total_loss\n",
        "    \n",
        "pre_trained_model = tf.keras.models.load_model( filepath='/content/drive/MyDrive/RSIC/NWPU-RESISC45/effb0_567_con_2loss_2/contrastive_model.h5', \n",
        "                                      custom_objects={'Contrastive_Loss_2': Contrastive_Loss_2})"
      ],
      "metadata": {
        "id": "8bLCkHrzvCZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load_or_create_model_for_pruning.py"
      ],
      "metadata": {
        "id": "keYqBo6pvMas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow_model_optimization as tfmot\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, GlobalAveragePooling1D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.layers import Conv2D, Activation, Dropout, Flatten, Input, Dense, MultiHeadAttention\n",
        "from tensorflow.keras.layers import Add, Average, Concatenate,Reshape, multiply, Permute, Lambda\n",
        "from tensorflow.keras import initializers, regularizers\n",
        "from tensorflow.keras.activations import sigmoid\n",
        "\n",
        "\n",
        "class Contrastive_Loss_2(tf.keras.losses.Loss):\n",
        "  def __init__(self, temperature=0.5, rate=0.5, name='Contrastive_Loss_2', **kwargs):\n",
        "    super(Contrastive_Loss_2, self).__init__(name=name, **kwargs)\n",
        "    self.temperature   = temperature\n",
        "    self.rate          = rate\n",
        "    self.cosine_sim    = tf.keras.losses.CosineSimilarity(axis=-1, reduction=tf.keras.losses.Reduction.NONE)\n",
        "    \n",
        "  # @tf.function\n",
        "  def call(self, z1, z2):\n",
        "    batch_size, n_dim = z1.shape\n",
        "\n",
        "    # Compute Euclid Distance loss\n",
        "    difference    = z1 - z2                                             # (BxB)   * z1 and z2 already applied soft max -> in the last axis, max dif will be 1 \n",
        "    squared_norm  = tf.reduce_sum(tf.square(difference), axis=1)        # (B)\n",
        "    distance      = tf.sqrt(squared_norm + 1e-8)                        # (B)     * + epsilon to avoid Nan in gradient\n",
        "    mean_distance = tf.reduce_mean(distance)                            # () -> scalar\n",
        "    tf.debugging.check_numerics(mean_distance.numpy(), 'Distance contains NaN values.')\n",
        "    # print('distance: , ',mean_distance)\n",
        "\n",
        "    # Compute Consine Similarity loss\n",
        "    z = tf.concat((z1, z2), 0)\n",
        "\n",
        "    sim_ij      = - self.cosine_sim(z[:batch_size], z[batch_size:])     # (B)  -> batch_size pair\n",
        "    sim_ji      = - self.cosine_sim(z[batch_size:], z[:batch_size])     # (B)  -> batch_size pair\n",
        "    sim_pos     = tf.concat((sim_ij,sim_ji), axis=0)                    # (2B) -> 2*batch_size positive pair\n",
        "    numerator   = tf.math.exp(sim_pos / self.temperature)               # (2B) -> 2*batch_size positive pair\n",
        "  \n",
        "    sim_neg     = - self.cosine_sim(tf.expand_dims(z, 1), z)            # sim (Bx1xE, BxE) -> (2Bx2B)\n",
        "    mask        = 1 - tf.eye(2*batch_size, dtype=tf.float32)            # (2Bx2B)\n",
        "    sim_neg     = mask * tf.math.exp(sim_neg / self.temperature)        # (2Bx2B)\n",
        "    denominator = tf.math.reduce_sum(sim_neg, axis=-1)                  # (2B) \n",
        "  \n",
        "    mean_cosine_similarity = tf.reduce_mean(- tf.math.log((numerator + 1e-11) / (denominator + 1e-11)))       # () -> scalar\n",
        "    tf.debugging.check_numerics(mean_cosine_similarity.numpy(), 'Cosine contains NaN values.')\n",
        "    # print('similarity: , ',mean_cosine_similarity)\n",
        "\n",
        "    # Compute total loss with associated rate\n",
        "    total_loss = (1-self.rate)*mean_distance + self.rate*mean_cosine_similarity \n",
        "    tf.debugging.check_numerics(total_loss.numpy(), 'Total contains NaN values.')\n",
        "    return total_loss\n",
        "\n",
        "def trippleAttention(x): # 8x8xc\n",
        "  ## TA - Tripple Attention\n",
        "  c = x.shape[-1]\n",
        "    # channel\n",
        "  tl1 = tf.math.reduce_mean(x, axis=-1) + tf.math.reduce_max(x, axis=-1)# 8x8        \n",
        "  tl1 = MultiHeadAttention(num_heads=16, key_dim=8)(tl1, tl1)           # 8x8 \n",
        "  tl1 = sigmoid(tl1)            # 8x8\n",
        "  tl1 = Reshape((8,8,1))(tl1)   # 8x8x1\n",
        "  tl1 = x * tl1                 # 8x8xc * 8x8x1 -> 8x8xc\n",
        "    # width\n",
        "  tl2 = tf.math.reduce_mean(x, axis=-2) + tf.math.reduce_max(x, axis=-2)# 8xc\n",
        "  tl2 = MultiHeadAttention(num_heads=16, key_dim=8)(tl2, tl2)          # 8xc\n",
        "  tl2 = sigmoid(tl2)            # 8xcx1\n",
        "  tl2 = Reshape((8,1,c))(tl2)   # 8x1xc\n",
        "  tl2 = x * tl2                 # 8x8xc * 8x1xc -> 8x8xc\n",
        "    # height\n",
        "  tl3 = tf.math.reduce_mean(x, axis=-3) + tf.math.reduce_max(x, axis=-3)# 8xc\n",
        "  tl3 = MultiHeadAttention(num_heads=16, key_dim=8)(tl3, tl3)          # 8xc\n",
        "  tl3 = sigmoid(tl3)            # 8xcx1\n",
        "  tl3 = Reshape((1,8,c))(tl3)   # 8x1xc\n",
        "  tl3 = x * tl3                 # 8x8xc * 1x8xc -> 8x8xc\n",
        "    # average \n",
        "  t = Average()([tl1, tl2, tl3]) # 8x8xc\n",
        "  t = GlobalAveragePooling2D(keepdims=False)(t) #channel\n",
        "  return t # c\n",
        "\n",
        "def get_model_architecture():\n",
        "  base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(256,256,3))\n",
        "\n",
        "  block7_x = base_model.output                           \n",
        "  block6_x = base_model.get_layer('block6d_add').output  \n",
        "  block5_x = base_model.get_layer('block5c_add').output  \n",
        "  \n",
        "  block6_x = Conv2D(filters=1280, kernel_size=1, strides=1)(block6_x) \n",
        "  block5_x = Conv2D(filters=1280, kernel_size=2, strides=2)(block5_x) \n",
        "  \n",
        "  block7_x = trippleAttention(block7_x)\n",
        "  block6_x = trippleAttention(block6_x)\n",
        "  block5_x = trippleAttention(block5_x)\n",
        "  \n",
        "  x = block5_x + block6_x + block7_x\n",
        "  x = Dense(512, \n",
        "                    activation='relu',\n",
        "                    kernel_initializer=initializers.TruncatedNormal(mean=0.0,stddev=0.1),\n",
        "                    kernel_regularizer=regularizers.l2(1e-5),\n",
        "                    bias_initializer=initializers.TruncatedNormal(mean=0.0, stddev=0.1),\n",
        "                    bias_regularizer=regularizers.l2(1e-5)\n",
        "                    )(x)\n",
        "  x = Dropout(0.2)(x)\n",
        "  predictions = Dense(45, \n",
        "                        activation='softmax',\n",
        "                        kernel_initializer=initializers.TruncatedNormal(mean=0.0,stddev=0.1),\n",
        "                        kernel_regularizer=regularizers.l2(1e-5),\n",
        "                        bias_initializer=initializers.TruncatedNormal(mean=0.0, stddev=0.1),\n",
        "                        bias_regularizer=regularizers.l2(1e-5)\n",
        "                        )(x)\n",
        "  return Model(base_model.input, predictions)\n",
        "\n",
        "# create full model\n",
        "def get_cls_model(pre_trained_model):\n",
        "  model = get_model_architecture()\n",
        "  model.set_weights(pre_trained_model.get_weights()) \n",
        "  return model\n",
        "\n",
        "def apply_pruning_to_dense(layer):\n",
        "  batch_size = 15\n",
        "  epochs     = 27\n",
        "  end_step   = (6300 / batch_size) * epochs\n",
        "  pruning_params = { 'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50, final_sparsity=0.75, begin_step=0, end_step=end_step)}\n",
        "\n",
        "  if isinstance(layer, tf.keras.layers.Dense):\n",
        "    return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)\n",
        "  if isinstance(layer, tf.keras.layers.Conv2D):\n",
        "    return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)\n",
        "  if isinstance(layer, tf.keras.layers.DepthwiseConv2D):\n",
        "    return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)\n",
        "  if isinstance(layer, tf.keras.layers.MultiHeadAttention):\n",
        "    return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)\n",
        "  if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
        "    return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)\n",
        "  return layer\n",
        "\n",
        "if os.path.exists('/content/drive/MyDrive/RSIC/NWPU-RESISC45/effb0_pruned/pruning_model.h5'):\n",
        "  print('loading model !')\n",
        "  model_for_pruning = tf.keras.models.load_model( filepath='/content/drive/MyDrive/RSIC/NWPU-RESISC45/effb0_pruned/pruning_model.h5', \n",
        "                                      custom_objects={'Contrastive_Loss_2': Contrastive_Loss_2})\n",
        "  print(model_for_pruning.loss)\n",
        "  print(model_for_pruning.optimizer.learning_rate)\n",
        "  # model_for_pruning.summary()\n",
        "else:\n",
        "  print('creating model !')\n",
        "  model_for_pruning = get_cls_model(pre_trained_model)\n",
        "\n",
        "  model_for_pruning = tf.keras.models.clone_model(model_for_pruning, clone_function=apply_pruning_to_dense,)\n",
        "  \n",
        "  # `prune_low_magnitude` requires a recompile.\n",
        "  opt = tf.keras.optimizers.Adam(learning_rate=5e-6)  \n",
        "  model_for_pruning.compile(optimizer=opt, loss=[tf.keras.losses.CategoricalCrossentropy(),Contrastive_Loss_2()], metrics=[])\n",
        "  # model_for_pruning.summary()\n"
      ],
      "metadata": {
        "id": "RivSh5jRvQdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine_tune_pruned_model.py"
      ],
      "metadata": {
        "id": "17CeWM_WvbBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "def Accuracy(y_true, y_pred):\n",
        "  if y_true.shape != y_pred.shape:\n",
        "    print('Error metric !')\n",
        "    raise ValueError('Something error in Acc calculation')\n",
        "  y_true   = tf.argmax(y_true, axis=-1) # (B) \n",
        "  y_pred   = tf.argmax(y_pred, axis=-1) # (B)\n",
        "  correct  = tf.cast(y_true == y_pred, tf.float32)\n",
        "  accuracy = tf.reduce_sum(correct)\n",
        "  return accuracy.numpy()\n",
        "\n",
        "def lr_schedule(epoch, lr):\n",
        "  if epoch < 5:\n",
        "    return 2e-6\n",
        "  elif epoch < 14:\n",
        "    return 1e-6 \n",
        "  elif epoch < 22:\n",
        "    return 6e-7\n",
        "  else:\n",
        "    return 1e-7\n",
        "\n",
        "TRAIN_DIR       = '/content/dataset2/train/'\n",
        "TEST_DIR        = '/content/dataset2/test/'\n",
        "stored_dir      = '/content/drive/MyDrive/RSIC/NWPU-RESISC45/effb0_pruned'\n",
        "best_model_file = '/content/drive/MyDrive/RSIC/NWPU-RESISC45/effb0_pruned/pruning_model.h5'\n",
        "BATCH_SIZE      = 15  # 3 or 5 or 9 or 15 or 45\n",
        "alpha           = 0.7 # tuning parameter\n",
        "current_epoch   = 0\n",
        "old_test_acc    = 0\n",
        "train_generator = TrainGenerator(img_dir=TRAIN_DIR, batch_size=BATCH_SIZE)\n",
        "test_generator  = TestGenerator(img_dir=TEST_DIR)\n",
        "\n",
        "# pruning param\n",
        "unused_arg = -1\n",
        "step_callback = tfmot.sparsity.keras.UpdatePruningStep()\n",
        "step_callback.set_model(model_for_pruning)\n",
        "step_callback.on_train_begin() \n",
        "\n",
        "for epoch in range(current_epoch, 27):\n",
        "  print('\\n\\n ==================== Epoch: ', epoch,'======================')\n",
        "  s = datetime.now()\n",
        "  train_acc = 0\n",
        "  epoch_loss = 0\n",
        "  label_dict = hypara().label_dict\n",
        "  class_list = list(label_dict.keys())\n",
        "  random.shuffle(class_list)\n",
        "\n",
        "  model_for_pruning.optimizer.learning_rate = lr_schedule(epoch, model_for_pruning.optimizer.learning_rate.numpy())\n",
        "  print(' *** learning rate: ', model_for_pruning.optimizer.learning_rate)\n",
        "  print('-------- training ---------')\n",
        "  for i in range(int(45 / BATCH_SIZE)):\n",
        "    for n_batch_train in range(140):\n",
        "      x_train, y_true_train = train_generator.get_batch(idx_num=n_batch_train, class_list=class_list[i*BATCH_SIZE:(i+1)*BATCH_SIZE], is_aug=True) # return 2 batches of images, each batch contain B images from B class\n",
        "      \n",
        "      step_callback.on_train_batch_begin(batch=unused_arg) # run pruning callback\n",
        "      with tf.GradientTape() as tape:\n",
        "        y_pred_train = model_for_pruning(x_train)\n",
        "        train_acc   += Accuracy(y_true_train, y_pred_train)\n",
        "        loss_1       = model_for_pruning.loss[0](y_true_train, y_pred_train)                            # categorical cross entropy\n",
        "        loss_2       = model_for_pruning.loss[1](y_pred_train[:BATCH_SIZE], y_pred_train[BATCH_SIZE:])  # euclid distance + consine similairy\n",
        "        loss         = alpha*loss_1 + (1-alpha)*loss_2                                      # total loss \n",
        "        epoch_loss  += loss \n",
        "        grads        = tape.gradient(loss, model_for_pruning.trainable_variables) \n",
        "        tf.debugging.check_numerics(grads[0], 'grad contains NaN values.')\n",
        "        model_for_pruning.optimizer.apply_gradients(zip(grads, model_for_pruning.trainable_variables))\n",
        "\n",
        "  train_acc /= 6300*2\n",
        "  print('# epoch loss:  ', epoch_loss.numpy(), '; epoch acc: ', train_acc)\n",
        "  \n",
        "  print(\"------ testing -------\")\n",
        "  if epoch >= 0: \n",
        "    start_test   = datetime.now()\n",
        "    test_acc     = 0\n",
        "    img_test     = 0\n",
        "    for n_batch_test in range(560):\n",
        "      x_test, y_true_test, n_imgs = test_generator.get_batch(n_batch_test)\n",
        "      y_pred_test         = model_for_pruning(x_test)\n",
        "      test_acc           += Accuracy(y_true_test, y_pred_test)\n",
        " \n",
        "    test_acc /= 25200\n",
        "    print('# test accuray: ', test_acc, '  and time needed for test: ', datetime.now()-start_test)\n",
        "       # Save model when successfully testing\n",
        "    if (test_acc > old_test_acc): \n",
        "      old_test_acc = test_acc\n",
        "      model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
        "      tf.keras.models.save_model(model_for_export, best_model_file, include_optimizer=True)\n",
        "      print('Save model completed')\n",
        "      with open(os.path.join(stored_dir,\"prunning_log.txt\"), \"a\") as text_file:\n",
        "        text_file.write(\"Save best model at Epoch: {}; Accuracy: {}\\n\".format(epoch, old_test_acc))\n",
        "\n",
        "  with open(os.path.join(stored_dir,\"prunning_log.txt\"), \"a\") as text_file:\n",
        "    text_file.write(\"Epoch: {}; lr: {}; Train accuracy: {}\\n\".format(epoch, model_for_pruning.optimizer.learning_rate.numpy(), train_acc))\n",
        "\n",
        "  step_callback.on_epoch_end(batch=unused_arg) # run pruning callback\n",
        "  print('# epoch training time: ', datetime.now()-s, '\\n')\n"
      ],
      "metadata": {
        "id": "pipXHoQKvfER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test quantization + pruning"
      ],
      "metadata": {
        "id": "j3Z8GFnXvkgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class Contrastive_Loss_2(tf.keras.losses.Loss):\n",
        "  def __init__(self, temperature=0.5, rate=0.5, name='Contrastive_Loss_2', **kwargs):\n",
        "    super(Contrastive_Loss_2, self).__init__(name=name, **kwargs)\n",
        "    self.temperature   = temperature\n",
        "    self.rate          = rate\n",
        "    self.cosine_sim    = tf.keras.losses.CosineSimilarity(axis=-1, reduction=tf.keras.losses.Reduction.NONE)\n",
        "    \n",
        "  # @tf.function\n",
        "  def call(self, z1, z2):\n",
        "    batch_size, n_dim = z1.shape\n",
        "\n",
        "    # Compute Euclid Distance loss\n",
        "    difference    = z1 - z2                                             # (BxB)   * z1 and z2 already applied soft max -> in the last axis, max dif will be 1 \n",
        "    squared_norm  = tf.reduce_sum(tf.square(difference), axis=1)        # (B)\n",
        "    distance      = tf.sqrt(squared_norm + 1e-8)                        # (B)     * + epsilon to avoid Nan in gradient\n",
        "    mean_distance = tf.reduce_mean(distance)                            # () -> scalar\n",
        "    tf.debugging.check_numerics(mean_distance.numpy(), 'Distance contains NaN values.')\n",
        "    # print('distance: , ',mean_distance)\n",
        "\n",
        "    # Compute Consine Similarity loss\n",
        "    z = tf.concat((z1, z2), 0)\n",
        "\n",
        "    sim_ij      = - self.cosine_sim(z[:batch_size], z[batch_size:])     # (B)  -> batch_size pair\n",
        "    sim_ji      = - self.cosine_sim(z[batch_size:], z[:batch_size])     # (B)  -> batch_size pair\n",
        "    sim_pos     = tf.concat((sim_ij,sim_ji), axis=0)                    # (2B) -> 2*batch_size positive pair\n",
        "    numerator   = tf.math.exp(sim_pos / self.temperature)               # (2B) -> 2*batch_size positive pair\n",
        "  \n",
        "    sim_neg     = - self.cosine_sim(tf.expand_dims(z, 1), z)            # sim (Bx1xE, BxE) -> (2Bx2B)\n",
        "    mask        = 1 - tf.eye(2*batch_size, dtype=tf.float32)            # (2Bx2B)\n",
        "    sim_neg     = mask * tf.math.exp(sim_neg / self.temperature)        # (2Bx2B)\n",
        "    denominator = tf.math.reduce_sum(sim_neg, axis=-1)                  # (2B) \n",
        "  \n",
        "    mean_cosine_similarity = tf.reduce_mean(- tf.math.log((numerator + 1e-11) / (denominator + 1e-11)))       # () -> scalar\n",
        "    tf.debugging.check_numerics(mean_cosine_similarity.numpy(), 'Cosine contains NaN values.')\n",
        "    # print('similarity: , ',mean_cosine_similarity)\n",
        "\n",
        "    # Compute total loss with associated rate\n",
        "    total_loss = (1-self.rate)*mean_distance + self.rate*mean_cosine_similarity \n",
        "    tf.debugging.check_numerics(total_loss.numpy(), 'Total contains NaN values.')\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "SjeJqrnavp8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "best_model = tf.keras.models.load_model( filepath='/content/drive/MyDrive/RSIC/NWPU-RESISC45/effb0_pruned/pruning_model.h5', \n",
        "                                      custom_objects={'Contrastive_Loss_2': Contrastive_Loss_2})\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(best_model) # quantize to 8 bit\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT] # int 8\n",
        "tflite_quant_prune_model = converter.convert()\n",
        "\n",
        "# #save converted quantization model to tflite format\n",
        "open(\"/content/drive/MyDrive/RSIC/NWPU-RESISC45/effb0_pruned/tflite_pruned_model.tflite\", \"wb\").write(tflite_quant_prune_model)\n",
        "\n",
        "\n",
        "best_model_path = '/content/drive/MyDrive/RSIC/NWPU-RESISC45/effb0_pruned/pruning_model.h5'\n",
        "quant_model_path = '/content/drive/MyDrive/RSIC/NWPU-RESISC45/effb0_pruned/tflite_pruned_model.tflite'\n",
        "\n",
        "print( os.path.getsize(best_model_path) / float(2**20))\n",
        "print( os.path.getsize(quant_model_path) / float(2**20))"
      ],
      "metadata": {
        "id": "QLl3cdpQvriy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}