{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install packages + check hardware"
      ],
      "metadata": {
        "id": "1yW8W_Cdr55H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nug7TGYOrix5"
      },
      "outputs": [],
      "source": [
        "! pip install tensorflow==2.9.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "restart runtime"
      ],
      "metadata": {
        "id": "bMOC5m_DsEZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_model_optimization"
      ],
      "metadata": {
        "id": "ETA3WlyWr1YT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "print(\"tensorflow version: \", tf.__version__) # 2.9.0\n",
        "!nvidia-smi -L"
      ],
      "metadata": {
        "id": "FwABcfzDr-5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset.py"
      ],
      "metadata": {
        "id": "pwoafDpNr_uu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! unrar x /content/drive/MyDrive/Dataset/NWPU-RESISC45.rar /content/\n",
        "import os \n",
        "print(len(os.listdir('/content/NWPU-RESISC45/')))\n",
        "print(len(os.listdir('/content/NWPU-RESISC45/airplane')))"
      ],
      "metadata": {
        "id": "wjX0R3ansM_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "input_dir = '/content/NWPU-RESISC45'\n",
        "store_dir = '/content/dataset2'\n",
        "\n",
        "# check/create directory\n",
        "if not os.path.exists(store_dir):\n",
        "  print('==== Create Data Dir =====')\n",
        "  os.makedirs(store_dir)\n",
        "\n",
        "store_dir_train = os.path.join(store_dir, 'train')\n",
        "if not os.path.exists(store_dir_train):\n",
        "  print('==== Create Train Dir =====')\n",
        "  os.makedirs(store_dir_train)\n",
        "\n",
        "store_dir_test = os.path.join(store_dir, 'test')\n",
        "if not os.path.exists(store_dir_test):\n",
        "  print('==== Create Test Dir =====')\n",
        "  os.makedirs(store_dir_test)\n",
        "\n",
        "class_list = os.listdir(input_dir)  # Name of all classes \n",
        "class_list.sort() # sort alphetically\n",
        "\n",
        "for class_name in class_list:\n",
        "  class_train_dir = os.path.abspath(os.path.join(store_dir_train, class_name))\n",
        "  os.makedirs(class_train_dir)\n",
        "  class_test_dir = os.path.abspath(os.path.join(store_dir_test, class_name))\n",
        "  os.makedirs(class_test_dir)\n",
        "\n",
        "  class_input_dir = os.path.abspath(os.path.join(input_dir, class_name))\n",
        "  file_name_list = os.listdir(class_input_dir)\n",
        "  file_name_list.sort()\n",
        "  # file_name_random = np.random.permutation(range(len(file_name_list)))\n",
        "\n",
        "  for i in range(len(file_name_list)):\n",
        "    # file_name = file_name_list[file_name_random[i]]\n",
        "    file_name = file_name_list[i]\n",
        "    file_open = os.path.join(class_input_dir, file_name)\n",
        "    org_img = Image.open(file_open)   # take out image\n",
        "    org_img = org_img.resize((256,256)) # resize\n",
        "    if i >= int(len(file_name_list)*0.8): # train set\n",
        "      train_path = os.path.join(class_train_dir, file_name)\n",
        "      org_img.save(train_path)\n",
        "    else:                                   # test  set\n",
        "      test_path = os.path.join(class_test_dir, file_name)\n",
        "      org_img.save(test_path)\n",
        "\n",
        "print(len(os.listdir('/content/dataset2/train/')))\n",
        "print(len(os.listdir('/content/dataset2/test/')))\n",
        "print(len(os.listdir('/content/dataset2/train/airplane')))\n",
        "print(len(os.listdir('/content/dataset2/test/airplane')))\n",
        "l = os.listdir('/content/dataset2/train/airplane')\n",
        "l.sort()\n",
        "print(l[0],l[-1])\n",
        "\n",
        "train_dir = '/content/dataset2/train'\n",
        "test_dir  = '/content/dataset2/test'\n",
        "train_count = 0\n",
        "test_count = 0\n",
        "class_list = os.listdir('/content/dataset2/train/')\n",
        "class_list.sort()\n",
        "for c in class_list:\n",
        "  train_path = os.path.join(train_dir,c)\n",
        "  test_path  = os.path.join(test_dir,c)\n",
        "  train_count += len(os.listdir(train_path))\n",
        "  test_count += len(os.listdir(test_path))\n",
        "  if len(os.listdir(train_path)) != 140 or len(os.listdir(test_path)) != 560:\n",
        "    print('something wrong on class: ', c)\n",
        "\n",
        "print(train_count, test_count, train_count + test_count)\n",
        "\n",
        "del train_count,test_count,train_dir,test_dir,l,org_img,file_name,file_open,file_name_list,class_list,class_input_dir,train_path,test_path,class_train_dir,class_test_dir"
      ],
      "metadata": {
        "id": "_RIatJ9hsO2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model.py"
      ],
      "metadata": {
        "id": "lYWc7z3ksSx2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model_1: ContrastiveLoss using probability distancce + cosine similarity"
      ],
      "metadata": {
        "id": "evS16q0esbhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pre_trained_model =  tf.keras.models.load_model('/content/drive/MyDrive/RSIC/NWPU-RESISC45/effb0_contrastive_2/contrastive_model.h5')"
      ],
      "metadata": {
        "id": "YRknRvRSsYil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, GlobalAveragePooling1D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.layers import Conv2D, Activation, Dropout, Flatten, Input, Dense, MultiHeadAttention\n",
        "from tensorflow.keras.layers import Add, Average, Concatenate,Reshape, multiply, Permute, Lambda\n",
        "from tensorflow.keras import initializers, regularizers\n",
        "from tensorflow.keras.activations import sigmoid\n",
        "\n",
        "class ContrastiveLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self, temperature=0.27, rate=0.5, name='ContrastiveLoss', **kwargs):\n",
        "    super(ContrastiveLoss, self).__init__(name=name, **kwargs)\n",
        "    self.temperature   = temperature\n",
        "    self.rate          = rate\n",
        "    self.cosine_sim    = tf.keras.losses.CosineSimilarity(axis=-1, reduction=tf.keras.losses.Reduction.NONE)\n",
        "    self.cross_entropy = tf.keras.losses.CategoricalCrossentropy(axis=-1, reduction=tf.keras.losses.Reduction.NONE)\n",
        "    \n",
        "  # @tf.function\n",
        "  def call(self, z1, z2):\n",
        "    batch_size, n_dim = z1.shape\n",
        "\n",
        "    # Compute Distance loss\n",
        "    z1_soft  = tf.keras.activations.softmax(z1, axis=-1)       # (Bx1280) \n",
        "    z2_soft  = tf.keras.activations.softmax(z2, axis=-1)       # (Bx1280) \n",
        "    distance = self.cross_entropy(z1_soft, z2_soft)            # (B) \n",
        "    mean_distance = tf.sqrt(tf.reduce_mean(distance))          # () -> scalar\n",
        "    tf.debugging.check_numerics(mean_distance.numpy(), 'Distance contains NaN values.')\n",
        "\n",
        "    # Compute Consine Similarity loss\n",
        "    z = tf.concat((z1, z2), 0)\n",
        "\n",
        "    sim_ij      = - self.cosine_sim(z[:batch_size], z[batch_size:])     # (B)  -> batch_size pair\n",
        "    sim_ji      = - self.cosine_sim(z[batch_size:], z[:batch_size])     # (B)  -> batch_size pair\n",
        "    sim_pos     = tf.concat((sim_ij,sim_ji), axis=0)                    # (2B) -> 2*batch_size positive pair\n",
        "    numerator   = tf.math.exp(sim_pos / self.temperature)               # (2B) -> 2*batch_size positive pair\n",
        "  \n",
        "    sim_neg     = - self.cosine_sim(tf.expand_dims(z, 1), z)            # sim (Bx1xE, BxE) -> (2Bx2B)\n",
        "    mask        = 1 - tf.eye(2*batch_size, dtype=tf.float32)            # (2Bx2B)\n",
        "    sim_neg     = mask * tf.math.exp(sim_neg / self.temperature)        # (2Bx2B)\n",
        "    denominator = tf.math.reduce_sum(sim_neg, axis=-1)                  # (2B) \n",
        "  \n",
        "    mean_cosine_similarity = tf.reduce_mean(- tf.math.log(numerator / denominator))       # () -> scalar\n",
        "    tf.debugging.check_numerics(mean_cosine_similarity.numpy(), 'Cosine contains NaN values.')\n",
        "\n",
        "    # Compute total loss with associated rate\n",
        "    total_loss = (1-self.rate)*mean_distance + self.rate*mean_cosine_similarity \n",
        "    tf.debugging.check_numerics(total_loss.numpy(), 'Total contains NaN values.')\n",
        "    return total_loss\n",
        "\n",
        "def trippleAttention(x): # 8x8xc\n",
        "  ## TA - Tripple Attention\n",
        "  c = x.shape[-1]\n",
        "    # channel\n",
        "  tl1 = tf.math.reduce_mean(x, axis=-1) + tf.math.reduce_max(x, axis=-1)# 8x8        \n",
        "  tl1 = MultiHeadAttention(num_heads=16, key_dim=8)(tl1, tl1)           # 8x8 \n",
        "  tl1 = sigmoid(tl1)            # 8x8\n",
        "  tl1 = Reshape((8,8,1))(tl1)   # 8x8x1\n",
        "  tl1 = x * tl1                 # 8x8xc * 8x8x1 -> 8x8xc\n",
        "    # width\n",
        "  tl2 = tf.math.reduce_mean(x, axis=-2) + tf.math.reduce_max(x, axis=-2)# 8xc\n",
        "  tl2 = MultiHeadAttention(num_heads=16, key_dim=8)(tl2, tl2)          # 8xc\n",
        "  tl2 = sigmoid(tl2)            # 8xcx1\n",
        "  tl2 = Reshape((8,1,c))(tl2)   # 8x1xc\n",
        "  tl2 = x * tl2                 # 8x8xc * 8x1xc -> 8x8xc\n",
        "    # height\n",
        "  tl3 = tf.math.reduce_mean(x, axis=-3) + tf.math.reduce_max(x, axis=-3)# 8xc\n",
        "  tl3 = MultiHeadAttention(num_heads=16, key_dim=8)(tl3, tl3)          # 8xc\n",
        "  tl3 = sigmoid(tl3)            # 8xcx1\n",
        "  tl3 = Reshape((1,8,c))(tl3)   # 8x1xc\n",
        "  tl3 = x * tl3                 # 8x8xc * 1x8xc -> 8x8xc\n",
        "    # average \n",
        "  t = Average()([tl1, tl2, tl3]) # 8x8xc\n",
        "  t = GlobalAveragePooling2D(keepdims=False)(t) #channel\n",
        "  return t # c\n",
        "\n",
        "def get_model_architecture():\n",
        "  base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(256,256,3))\n",
        "\n",
        "  block7_x = base_model.output                           \n",
        "  block6_x = base_model.get_layer('block6d_add').output  \n",
        "  block5_x = base_model.get_layer('block5c_add').output  \n",
        "  \n",
        "  block6_x = Conv2D(filters=1280, kernel_size=1, strides=1)(block6_x) \n",
        "  block5_x = Conv2D(filters=1280, kernel_size=2, strides=2)(block5_x) \n",
        "  \n",
        "  block7_x = trippleAttention(block7_x)\n",
        "  block6_x = trippleAttention(block6_x)\n",
        "  block5_x = trippleAttention(block5_x)\n",
        "  \n",
        "  x = block5_x + block6_x + block7_x\n",
        "  x = Dense(512, \n",
        "                    activation='relu',\n",
        "                    kernel_initializer=initializers.TruncatedNormal(mean=0.0,stddev=0.1),\n",
        "                    kernel_regularizer=regularizers.l2(1e-5),\n",
        "                    bias_initializer=initializers.TruncatedNormal(mean=0.0, stddev=0.1),\n",
        "                    bias_regularizer=regularizers.l2(1e-5)\n",
        "                    )(x)\n",
        "  x = Dropout(0.2)(x)\n",
        "  predictions = Dense(45, \n",
        "                        activation='softmax',\n",
        "                        kernel_initializer=initializers.TruncatedNormal(mean=0.0,stddev=0.1),\n",
        "                        kernel_regularizer=regularizers.l2(1e-5),\n",
        "                        bias_initializer=initializers.TruncatedNormal(mean=0.0, stddev=0.1),\n",
        "                        bias_regularizer=regularizers.l2(1e-5)\n",
        "                        )(x)\n",
        "  return Model(base_model.input, predictions)\n",
        "\n",
        "# create full model\n",
        "def get_cls_model():\n",
        "  model = get_model_architecture()\n",
        "  model.set_weights(pre_trained_model.get_weights()) \n",
        "  model.trainable = True \n",
        "  opt = tf.keras.optimizers.Adam(learning_rate=1e-6) \n",
        "  model.compile(loss=[tf.keras.losses.CategoricalCrossentropy(),ContrastiveLoss()], optimizer=opt, metrics=[])   \n",
        "  return model\n",
        "\n",
        "if os.path.exists('/content/drive/MyDrive/RSIC/NWPU-RESISC45/effb0_567_con_2loss/contrastive_model.h5'):\n",
        "  print('loading model !')\n",
        "  model = tf.keras.models.load_model( filepath='/content/drive/MyDrive/RSIC/NWPU-RESISC45/effb0_567_con_2loss/contrastive_model.h5', \n",
        "                                      custom_objects={'ContrastiveLoss': ContrastiveLoss})\n",
        "  print(model.loss)\n",
        "  print(model.optimizer.learning_rate)\n",
        "  # model.summary()\n",
        "else:\n",
        "  print('creating model !')\n",
        "  model = get_cls_model()\n",
        "  model.summary()\n"
      ],
      "metadata": {
        "id": "twuMJA-Dse4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model_2: ContrastiveLoss using Euclid distancce + cosine similarity"
      ],
      "metadata": {
        "id": "JjI01198srTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class ContrastiveLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self, temperature=0.27, rate=0.72, name='ContrastiveLoss', **kwargs):\n",
        "    super(ContrastiveLoss, self).__init__(name=name, **kwargs)\n",
        "    self.temperature   = temperature\n",
        "    self.rate          = rate\n",
        "    self.cosine_sim    = tf.keras.losses.CosineSimilarity(axis=-1, reduction=tf.keras.losses.Reduction.NONE)\n",
        "    self.cross_entropy = tf.keras.losses.CategoricalCrossentropy(axis=-1, reduction=tf.keras.losses.Reduction.NONE)\n",
        "    \n",
        "  # @tf.function\n",
        "  def call(self, z1, z2):\n",
        "    batch_size, n_dim = z1.shape\n",
        "\n",
        "    # Compute Distance loss\n",
        "    z1_soft = tf.keras.activations.softmax(z1, axis=-1)       # (Bx1280) \n",
        "    z2_soft = tf.keras.activations.softmax(z2, axis=-1)       # (Bx1280) \n",
        "    distance = self.cross_entropy(z1_soft, z2_soft)           # (B) \n",
        "    mean_distance = tf.sqrt(tf.reduce_mean(distance))         # () -> scalar\n",
        "    # print('dis: ', mean_distance)\n",
        "    tf.debugging.check_numerics(mean_distance.numpy(), 'Distance contains NaN values.')\n",
        "\n",
        "    # Compute Consine Similarity loss\n",
        "    z = tf.concat((z1, z2), 0)\n",
        "\n",
        "    sim_ij      = - self.cosine_sim(z[:batch_size], z[batch_size:])     # (B)  -> batch_size pair\n",
        "    sim_ji      = - self.cosine_sim(z[batch_size:], z[:batch_size])     # (B)  -> batch_size pair\n",
        "    sim_pos     = tf.concat((sim_ij,sim_ji), axis=0)                    # (2B) -> 2*batch_size positive pair\n",
        "    numerator   = tf.math.exp(sim_pos / self.temperature)               # (2B) -> 2*batch_size positive pair\n",
        "    # print(numerator)\n",
        "    sim_neg     = - self.cosine_sim(tf.expand_dims(z, 1), z)            # sim (Bx1xE, BxE) -> (2Bx2B)\n",
        "    mask        = 1 - tf.eye(2*batch_size, dtype=tf.float32)            # (2Bx2B)\n",
        "    sim_neg     = mask * tf.math.exp(sim_neg / self.temperature)        # (2Bx2B)\n",
        "    denominator = tf.math.reduce_sum(sim_neg, axis=-1)                  # (2B) \n",
        "    # print(denominator)\n",
        "    mean_cosine_similarity = tf.reduce_mean(- tf.math.log(numerator / denominator))       # () -> scalar\n",
        "    tf.debugging.check_numerics(mean_cosine_similarity.numpy(), 'Cosine contains NaN values.')\n",
        "    # print('cos: ', mean_cosine_similarity)\n",
        "\n",
        "    # Compute total loss with associated rate\n",
        "    total_loss = (1-self.rate)*mean_distance + self.rate*mean_cosine_similarity \n",
        "    # print('total: ',total_loss)\n",
        "    tf.debugging.check_numerics(total_loss.numpy(), 'Total contains NaN values.')\n",
        "    return total_loss\n",
        "\n",
        "pre_trained_model =  tf.keras.models.load_model('/content/drive/MyDrive/RSIC/NWPU-RESISC45/effb0_567_con_2loss/contrastive_model.h5',custom_objects={'ContrastiveLoss': ContrastiveLoss})\n",
        "# pre_trained_model = tf.keras.models.load_model('/content/drive/MyDrive/RSIC/NWPU-RESISC45/effb0_contrastive_2/contrastive_model.h5')"
      ],
      "metadata": {
        "id": "_8DGYtjSstCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, GlobalAveragePooling1D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.layers import Conv2D, Activation, Dropout, Flatten, Input, Dense, MultiHeadAttention\n",
        "from tensorflow.keras.layers import Add, Average, Concatenate,Reshape, multiply, Permute, Lambda\n",
        "from tensorflow.keras import initializers, regularizers\n",
        "from tensorflow.keras.activations import sigmoid\n",
        "\n",
        "class Contrastive_Loss_2(tf.keras.losses.Loss):\n",
        "  def __init__(self, temperature=0.5, rate=0.5, name='Contrastive_Loss_2', **kwargs):\n",
        "    super(Contrastive_Loss_2, self).__init__(name=name, **kwargs)\n",
        "    self.temperature   = temperature\n",
        "    self.rate          = rate\n",
        "    self.cosine_sim    = tf.keras.losses.CosineSimilarity(axis=-1, reduction=tf.keras.losses.Reduction.NONE)\n",
        "    \n",
        "  # @tf.function\n",
        "  def call(self, z1, z2):\n",
        "    batch_size, n_dim = z1.shape\n",
        "\n",
        "    # Compute Euclid Distance loss\n",
        "    difference    = z1 - z2                                             # (BxB)   * z1 and z2 already applied soft max -> in the last axis, max dif will be 1 \n",
        "    squared_norm  = tf.reduce_sum(tf.square(difference), axis=1)        # (B)\n",
        "    distance      = tf.sqrt(squared_norm + 1e-8)                        # (B)     * + epsilon to avoid Nan in gradient\n",
        "    mean_distance = tf.reduce_mean(distance)                            # () -> scalar\n",
        "    tf.debugging.check_numerics(mean_distance.numpy(), 'Distance contains NaN values.')\n",
        "    # print('distance: , ',mean_distance)\n",
        "\n",
        "    # Compute Consine Similarity loss\n",
        "    z = tf.concat((z1, z2), 0)\n",
        "\n",
        "    sim_ij      = - self.cosine_sim(z[:batch_size], z[batch_size:])     # (B)  -> batch_size pair\n",
        "    sim_ji      = - self.cosine_sim(z[batch_size:], z[:batch_size])     # (B)  -> batch_size pair\n",
        "    sim_pos     = tf.concat((sim_ij,sim_ji), axis=0)                    # (2B) -> 2*batch_size positive pair\n",
        "    numerator   = tf.math.exp(sim_pos / self.temperature)               # (2B) -> 2*batch_size positive pair\n",
        "  \n",
        "    sim_neg     = - self.cosine_sim(tf.expand_dims(z, 1), z)            # sim (Bx1xE, BxE) -> (2Bx2B)\n",
        "    mask        = 1 - tf.eye(2*batch_size, dtype=tf.float32)            # (2Bx2B)\n",
        "    sim_neg     = mask * tf.math.exp(sim_neg / self.temperature)        # (2Bx2B)\n",
        "    denominator = tf.math.reduce_sum(sim_neg, axis=-1)                  # (2B) \n",
        "  \n",
        "    mean_cosine_similarity = tf.reduce_mean(- tf.math.log((numerator + 1e-11) / (denominator + 1e-11)))       # () -> scalar\n",
        "    tf.debugging.check_numerics(mean_cosine_similarity.numpy(), 'Cosine contains NaN values.')\n",
        "    # print('similarity: , ',mean_cosine_similarity)\n",
        "\n",
        "    # Compute total loss with associated rate\n",
        "    total_loss = (1-self.rate)*mean_distance + self.rate*mean_cosine_similarity \n",
        "    tf.debugging.check_numerics(total_loss.numpy(), 'Total contains NaN values.')\n",
        "    return total_loss\n",
        "\n",
        "def trippleAttention(x): # 8x8xc\n",
        "  ## TA - Tripple Attention\n",
        "  c = x.shape[-1]\n",
        "    # channel\n",
        "  tl1 = tf.math.reduce_mean(x, axis=-1) + tf.math.reduce_max(x, axis=-1)# 8x8        \n",
        "  tl1 = MultiHeadAttention(num_heads=16, key_dim=8)(tl1, tl1)           # 8x8 \n",
        "  tl1 = sigmoid(tl1)            # 8x8\n",
        "  tl1 = Reshape((8,8,1))(tl1)   # 8x8x1\n",
        "  tl1 = x * tl1                 # 8x8xc * 8x8x1 -> 8x8xc\n",
        "    # width\n",
        "  tl2 = tf.math.reduce_mean(x, axis=-2) + tf.math.reduce_max(x, axis=-2)# 8xc\n",
        "  tl2 = MultiHeadAttention(num_heads=16, key_dim=8)(tl2, tl2)          # 8xc\n",
        "  tl2 = sigmoid(tl2)            # 8xcx1\n",
        "  tl2 = Reshape((8,1,c))(tl2)   # 8x1xc\n",
        "  tl2 = x * tl2                 # 8x8xc * 8x1xc -> 8x8xc\n",
        "    # height\n",
        "  tl3 = tf.math.reduce_mean(x, axis=-3) + tf.math.reduce_max(x, axis=-3)# 8xc\n",
        "  tl3 = MultiHeadAttention(num_heads=16, key_dim=8)(tl3, tl3)          # 8xc\n",
        "  tl3 = sigmoid(tl3)            # 8xcx1\n",
        "  tl3 = Reshape((1,8,c))(tl3)   # 8x1xc\n",
        "  tl3 = x * tl3                 # 8x8xc * 1x8xc -> 8x8xc\n",
        "    # average \n",
        "  t = Average()([tl1, tl2, tl3]) # 8x8xc\n",
        "  t = GlobalAveragePooling2D(keepdims=False)(t) #channel\n",
        "  return t # c\n",
        "\n",
        "def get_model_architecture():\n",
        "  base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(256,256,3))\n",
        "\n",
        "  block7_x = base_model.output                           \n",
        "  block6_x = base_model.get_layer('block6d_add').output  \n",
        "  block5_x = base_model.get_layer('block5c_add').output  \n",
        "  \n",
        "  block6_x = Conv2D(filters=1280, kernel_size=1, strides=1)(block6_x) \n",
        "  block5_x = Conv2D(filters=1280, kernel_size=2, strides=2)(block5_x) \n",
        "  \n",
        "  block7_x = trippleAttention(block7_x)\n",
        "  block6_x = trippleAttention(block6_x)\n",
        "  block5_x = trippleAttention(block5_x)\n",
        "  \n",
        "  x = block5_x + block6_x + block7_x\n",
        "  x = Dense(512, \n",
        "                    activation='relu',\n",
        "                    kernel_initializer=initializers.TruncatedNormal(mean=0.0,stddev=0.1),\n",
        "                    kernel_regularizer=regularizers.l2(1e-5),\n",
        "                    bias_initializer=initializers.TruncatedNormal(mean=0.0, stddev=0.1),\n",
        "                    bias_regularizer=regularizers.l2(1e-5)\n",
        "                    )(x)\n",
        "  x = Dropout(0.2)(x)\n",
        "  predictions = Dense(45, \n",
        "                        activation='softmax',\n",
        "                        kernel_initializer=initializers.TruncatedNormal(mean=0.0,stddev=0.1),\n",
        "                        kernel_regularizer=regularizers.l2(1e-5),\n",
        "                        bias_initializer=initializers.TruncatedNormal(mean=0.0, stddev=0.1),\n",
        "                        bias_regularizer=regularizers.l2(1e-5)\n",
        "                        )(x)\n",
        "  return Model(base_model.input, predictions)\n",
        "\n",
        "# create full model\n",
        "def get_cls_model(pre_trained_model):\n",
        "  model = get_model_architecture()\n",
        "  model.set_weights(pre_trained_model.get_weights()) \n",
        "  model.trainable = True \n",
        "  opt = tf.keras.optimizers.Adam(learning_rate=1e-7)  \n",
        "  model.compile(loss=[tf.keras.losses.CategoricalCrossentropy(),Contrastive_Loss_2()], optimizer=opt, metrics=[])   \n",
        "  return model\n",
        "\n",
        "if os.path.exists('/content/drive/MyDrive/RSIC/NWPU-RESISC45/effb0_567_con_2loss_2/contrastive_model.h5'):\n",
        "  print('loading model !')\n",
        "  model = tf.keras.models.load_model( filepath='/content/drive/MyDrive/RSIC/NWPU-RESISC45/effb0_567_con_2loss_2/contrastive_model.h5', \n",
        "                                      custom_objects={'Contrastive_Loss_2': Contrastive_Loss_2})\n",
        "  print(model.loss)\n",
        "  print(model.optimizer.learning_rate)\n",
        "  # model.summary()\n",
        "else:\n",
        "  print('creating model !')\n",
        "  model = get_cls_model(pre_trained_model)\n",
        "  # model.summary()\n"
      ],
      "metadata": {
        "id": "Z1_-YFvssyRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyper-parameter.py"
      ],
      "metadata": {
        "id": "b3MgGxwQs__k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class hypara(object):\n",
        "  def __init__(self):\n",
        "    self.label_dict  = dict(    airplane              = 0,\n",
        "                                airport               = 1,\n",
        "                                baseball_diamond      = 2,\n",
        "                                basketball_court      = 3,\n",
        "                                beach                 = 4,\n",
        "                                bridge                = 5,\n",
        "                                chaparral             = 6,\n",
        "                                church                = 7,\n",
        "                                circular_farmland     = 8,\n",
        "                                cloud                 = 9,\n",
        "                                commercial_area       = 10,\n",
        "                                dense_residential     = 11,\n",
        "                                desert                = 12,\n",
        "                                forest                = 13,\n",
        "                                freeway               = 14,\n",
        "                                golf_course           = 15,\n",
        "                                ground_track_field    = 16,\n",
        "                                harbor                = 17,\n",
        "                                industrial_area       = 18,\n",
        "                                intersection          = 19,\n",
        "                                island                = 20,\n",
        "                                lake                  = 21,\n",
        "                                meadow                = 22,\n",
        "                                medium_residential    = 23,\n",
        "                                mobile_home_park      = 24,\n",
        "                                mountain              = 25,\n",
        "                                overpass              = 26,\n",
        "                                palace                = 27,\n",
        "                                parking_lot           = 28,\n",
        "                                railway               = 29,\n",
        "                                railway_station       = 30,\n",
        "                                rectangular_farmland  = 31,\n",
        "                                river                 = 32,\n",
        "                                roundabout            = 33,\n",
        "                                runway                = 34,\n",
        "                                sea_ice               = 35,\n",
        "                                ship                  = 36,\n",
        "                                snowberg              = 37,\n",
        "                                sparse_residential    = 38,\n",
        "                                stadium               = 39,\n",
        "                                storage_tank          = 40,\n",
        "                                tennis_court          = 41,\n",
        "                                terrace               = 42,\n",
        "                                thermal_power_station = 43,\n",
        "                                wetland               = 44 )\n",
        "\n",
        "        #---- Para for generator and training\n",
        "    self.W             = 256   \n",
        "    self.H             = 256  \n",
        "    self.C             = 3    \n",
        "    self.batch_size    = 45 \n",
        "    self.learning_rate = 1e-6\n",
        "    self.check_every   = 200\n",
        "    self.class_num     = 45\n",
        "    self.epoch_num     = 273"
      ],
      "metadata": {
        "id": "-5BjhAvotDW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generator.py"
      ],
      "metadata": {
        "id": "0j5-LNj1tMCP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TrainGenerator"
      ],
      "metadata": {
        "id": "SGrlTWQ5tQ_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image, ImageFilter, ImageEnhance\n",
        "import os\n",
        "\n",
        "class TrainGenerator(object):\n",
        "  def __init__(self, img_dir, batch_size=15):  \n",
        "    self.img_dir       = img_dir # 'content/dataset2/train/'\n",
        "    self.batch_size    = batch_size\n",
        "    self.class_num     = hypara().class_num\n",
        "    self.label_dict    = hypara().label_dict\n",
        "    \n",
        "  def get_num_classes(self):\n",
        "    return len(os.listdir(self.img_dir))\n",
        "\n",
        "  def get_imgs_of_class(self,cls_name):\n",
        "    return os.listdir(os.path.join(self.img_dir,cls_name))\n",
        "\n",
        "  def get_batch (self, idx_num, class_list, is_aug=True):\n",
        "    if self.batch_size != len(class_list):\n",
        "      raise ValueError('batch_size is not equal !')\n",
        "    \n",
        "    x = np.zeros((self.batch_size*2,256,256,3), dtype=np.float32)\n",
        "    y = np.zeros((self.batch_size*2,self.class_num), dtype=np.float32)\n",
        "         \n",
        "    i = 0\n",
        "    for c in class_list: # class_list can be considered as batch_size\n",
        "      file_path     = os.path.join(self.img_dir, c)\n",
        "      file_list     = self.get_imgs_of_class(c)\n",
        "\n",
        "      file_name_1   = file_list[idx_num]\n",
        "      file_name_2   = file_list[random.randrange(140)]\n",
        "\n",
        "      file_open_1   = os.path.join(file_path, file_name_1)\n",
        "      file_open_2   = os.path.join(file_path, file_name_2)\n",
        "     \n",
        "      img_1         = Image.open(file_open_1) \n",
        "      img_2         = Image.open(file_open_2) \n",
        "\n",
        "      if is_aug == True:\n",
        "        img_1       = self.augmentate(img_1)\n",
        "        img_2       = self.augmentate(img_2) \n",
        "\n",
        "      img_1 = np.asarray(img_1).astype(np.float32) # 256x256x3\n",
        "      img_2 = np.asarray(img_2).astype(np.float32) # 256x256x3\n",
        "\n",
        "      # img_1 /= 255\n",
        "      # img_2 /= 255\n",
        "          \n",
        "      x[i,:,:,:] = img_1\n",
        "      x[i+self.batch_size,:,:,:] = img_2\n",
        "\n",
        "      class_idx  = self.label_dict[c]\n",
        "      y[i, class_idx] = 1\n",
        "      y[i+self.batch_size, class_idx] = 1\n",
        "\n",
        "      i += 1\n",
        "\n",
        "    x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
        "    y = tf.convert_to_tensor(y, dtype=tf.float32)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "  def augmentate(self, one_image): #---- data augmentation\n",
        "    if np.random.uniform() > 0.6: # not augmentation\n",
        "      return one_image\n",
        "\n",
        "    if np.random.uniform() > 0.5: # rotate\n",
        "      angle = random.choice([15,90,180,270])\n",
        "      one_image  = one_image.rotate(angle)\n",
        "    \n",
        "    if np.random.uniform() > 0.5: # crop\n",
        "      rand = np.random.randint(0,15)\n",
        "      one_image  = one_image.crop((rand,rand,256-rand,256-rand))\n",
        "      one_image  = one_image.resize((256,256), resample=Image.BILINEAR)\n",
        "    \n",
        "    rand = np.random.uniform()\n",
        "    if rand > 0.5: # change color channel\n",
        "      r, g, b = one_image.split()\n",
        "      tup =  (b, r, g) if rand > 0.75 else (g, b, r)\n",
        "      one_image = Image.merge(\"RGB\", tup)\n",
        "    \n",
        "    if np.random.uniform() > 0.5: # change brightness\n",
        "      one_image = one_image.point(lambda i: i * 1.17 if i*1.17 <= 255 else 255)\n",
        "    \n",
        "    if np.random.uniform() > 0.5: # add noise\n",
        "      one_image = one_image.point(lambda i: i + np.random.randint(-4,4)) \n",
        "\n",
        "    rand = np.random.uniform() \n",
        "    if rand > 0.75: # change contrast/sharpness\n",
        "      enhancer = ImageEnhance.Sharpness(one_image)\n",
        "      one_image = enhancer.enhance(2.73)\n",
        "    elif rand > 0.5:\n",
        "      enhancer = ImageEnhance.Contrast(one_image)\n",
        "      one_image = enhancer.enhance(2.27)\n",
        "\n",
        "    return one_image\n",
        "\n",
        "img_dir = '/content/dataset2/train/'\n",
        "generator = TrainGenerator(img_dir=img_dir, batch_size=5)\n",
        "\n",
        "print(generator.get_num_classes())\n",
        "print(len(generator.get_imgs_of_class('roundabout')))\n",
        "\n",
        "cls = ['chaparral', 'runway', 'sea_ice', 'intersection', 'desert']\n",
        "x, y = generator.get_batch(0,cls,True)\n",
        "print(x.shape,y.shape)\n",
        "print(tf.argmax(y,axis=-1))"
      ],
      "metadata": {
        "id": "wvwvIo3RtM23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = Image.fromarray((x[0].numpy()).astype(np.uint8), 'RGB') # *255\n",
        "img.show()\n",
        "img = Image.fromarray((x[0+5].numpy()).astype(np.uint8), 'RGB') \n",
        "img.show()\n",
        "img = Image.fromarray((x[1].numpy()).astype(np.uint8), 'RGB')\n",
        "img.show()\n",
        "img = Image.fromarray((x[1+5].numpy()).astype(np.uint8), 'RGB')\n",
        "img.show()\n",
        "img = Image.fromarray((x[2].numpy()).astype(np.uint8), 'RGB')\n",
        "img.show()\n",
        "img = Image.fromarray((x[2+5].numpy()).astype(np.uint8), 'RGB')\n",
        "img.show()\n",
        "\n",
        "del img, x, y, generator, img_dir, cls"
      ],
      "metadata": {
        "id": "gKWgVqHBtWqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TestGenerator"
      ],
      "metadata": {
        "id": "vNR4BR6ctXl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image, ImageFilter, ImageEnhance\n",
        "import os\n",
        "\n",
        "class TestGenerator(object):\n",
        "  def __init__(self, img_dir):  \n",
        "    self.img_dir       = img_dir # 'content/dataset2/test/'\n",
        "    self.batch_size    = 45\n",
        "    self.class_num     = hypara().class_num\n",
        "    self.label_dict    = hypara().label_dict\n",
        "    self.class_list    = list(self.label_dict.keys())\n",
        "\n",
        "  def get_class_list(self):\n",
        "    return self.class_list\n",
        "\n",
        "  def get_num_classes(self):\n",
        "    return self.class_num, len(self.class_list)\n",
        "\n",
        "  def get_imgs_of_class(self, cls):\n",
        "    file_list = os.listdir(os.path.join(self.img_dir, cls))\n",
        "    file_list.sort()\n",
        "    return file_list\n",
        "\n",
        "  def get_batch (self, idx): \n",
        "    if self.class_num != len(self.class_list):\n",
        "      raise ValueError('batch_size != 45')\n",
        "\n",
        "    x = np.zeros((self.batch_size,256,256,3), dtype=np.float32)\n",
        "    y = np.zeros((self.batch_size,self.class_num), dtype=np.float32)\n",
        "\n",
        "    i = 0\n",
        "    for c in self.class_list:\n",
        "      file_path   = os.path.join(self.img_dir, c) \n",
        "      file_list   = self.get_imgs_of_class(c)\n",
        "      file_name   = file_list[idx]\n",
        "      file_open   = os.path.join(file_path, file_name)\n",
        "      one_image   = Image.open(file_open) \n",
        "      one_image   = np.asarray(one_image).astype(np.float32) # 256x256x3\n",
        "      # one_image  /= 255\n",
        "      class_idx   = self.label_dict[c]\n",
        "\n",
        "      x[i,:,:,:]  = one_image\n",
        "      y[i, class_idx] = 1\n",
        "      i += 1\n",
        "\n",
        "    x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
        "    y = tf.convert_to_tensor(y, dtype=tf.float32)\n",
        "\n",
        "    return x, y, i\n",
        "\n",
        "test_dir = '/content/dataset2/test/'\n",
        "test_generator = TestGenerator(img_dir=test_dir)\n",
        "\n",
        "print(test_generator.get_num_classes())\n",
        "print(test_generator.get_class_list())\n",
        "print(len(test_generator.get_imgs_of_class('roundabout')),len(test_generator.get_imgs_of_class('bridge')))\n",
        "\n",
        "x, y, n_imgs = test_generator.get_batch(idx=1)\n",
        "\n",
        "print(x.shape,y.shape, n_imgs)\n",
        "print(tf.argmax(y,axis=-1))"
      ],
      "metadata": {
        "id": "5nAm1fPotZ-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = Image.fromarray((x[0].numpy()).astype(np.uint8), 'RGB') # *255\n",
        "img.show()\n",
        "img = Image.fromarray((x[1].numpy()).astype(np.uint8), 'RGB') \n",
        "img.show()\n",
        "img = Image.fromarray((x[2].numpy()).astype(np.uint8), 'RGB')\n",
        "img.show()\n",
        "img = Image.fromarray((x[3].numpy()).astype(np.uint8), 'RGB')\n",
        "img.show()\n",
        "img = Image.fromarray((x[4].numpy()).astype(np.uint8), 'RGB')\n",
        "img.show()\n",
        "img = Image.fromarray((x[5].numpy()).astype(np.uint8), 'RGB')\n",
        "img.show()\n",
        "\n",
        "del img, x, y, test_generator, test_dir"
      ],
      "metadata": {
        "id": "v2_D2YP1tcqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train.py"
      ],
      "metadata": {
        "id": "4lh6oIhBtfoz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train.py for model_1"
      ],
      "metadata": {
        "id": "A-ND9YSRtotw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "def Accuracy(y_true, y_pred):\n",
        "  if y_true.shape != y_pred.shape:\n",
        "    print('Error metric !')\n",
        "    raise ValueError('Something error in Acc calculation')\n",
        "  y_true   = tf.argmax(y_true, axis=-1) # (B) \n",
        "  y_pred   = tf.argmax(y_pred, axis=-1) # (B)\n",
        "  correct  = tf.cast(y_true == y_pred, tf.float32)\n",
        "  accuracy = tf.reduce_sum(correct)\n",
        "  return accuracy.numpy()\n",
        "\n",
        "def lr_schedule(epoch, lr):\n",
        "  if epoch < 25:\n",
        "    return 1e-6\n",
        "  elif epoch < 35:\n",
        "    return 7.2e-7 \n",
        "  elif epoch < 45:\n",
        "    return 5e-7\n",
        "  elif epoch < 55:\n",
        "    return 2e-6\n",
        "  elif epoch < 96:\n",
        "    return 1e-6\n",
        "  else:\n",
        "    return 9e-7\n",
        "\n",
        "TRAIN_DIR       = '/content/dataset2/train/'\n",
        "TEST_DIR        = '/content/dataset2/test/'\n",
        "stored_dir      = '/content/drive/MyDrive/RSIC/NWPU-RESISC45/effb0_567_con_2loss'\n",
        "best_model_file = '/content/drive/MyDrive/RSIC/NWPU-RESISC45/effb0_567_con_2loss/contrastive_model.h5'\n",
        "BATCH_SIZE      = 15 # 3 or 5 or 9 or 15 or 45\n",
        "alpha           = 0.72 # tuning parameter\n",
        "current_epoch   = 116\n",
        "old_test_acc    = 0.9381746031746032\n",
        "train_generator = TrainGenerator(img_dir=TRAIN_DIR, batch_size=BATCH_SIZE)\n",
        "test_generator  = TestGenerator(img_dir=TEST_DIR)\n",
        "\n",
        "\n",
        "for epoch in range(current_epoch, 127):\n",
        "  print('\\n\\n ==================== Epoch: ', epoch,'======================')\n",
        "  s = datetime.now()\n",
        "  train_acc = 0\n",
        "  epoch_loss = 0\n",
        "  label_dict = hypara().label_dict\n",
        "  class_list = list(label_dict.keys())\n",
        "  random.shuffle(class_list)\n",
        "\n",
        "  # if epoch == current_epoch:\n",
        "  #   model.optimizer.learning_rate = 1e-6\n",
        "  # else:  \n",
        "  model.optimizer.learning_rate = lr_schedule(epoch, model.optimizer.learning_rate.numpy())\n",
        "  print(' *** learning rate: ', model.optimizer.learning_rate)\n",
        "  print('-------- training ---------')\n",
        "  for i in range(int(45 / BATCH_SIZE)):\n",
        "    for n_batch_train in range(140):\n",
        "      x_train, y_true_train = train_generator.get_batch(idx_num=n_batch_train, class_list=class_list[i*BATCH_SIZE:(i+1)*BATCH_SIZE], is_aug=True) # return 2 batches of images, each batch contain B images from B class\n",
        "      with tf.GradientTape() as tape:\n",
        "        y_pred_train = model(x_train)\n",
        "        train_acc   += Accuracy(y_true_train, y_pred_train)\n",
        "        # print(Accuracy(y_true_train, y_pred_train)/30)\n",
        "        loss_1       = model.loss[0](y_true_train, y_pred_train)                            # categorical cross entropy\n",
        "        loss_2       = model.loss[1](y_pred_train[:BATCH_SIZE], y_pred_train[BATCH_SIZE:])  # probability distance + consine similairy\n",
        "        loss         = alpha*loss_1 + (1-alpha)*loss_2                                      # total loss \n",
        "        epoch_loss  += loss \n",
        "        grads        = tape.gradient(loss, model.trainable_variables) \n",
        "        tf.debugging.check_numerics(grads[0], 'grad contains NaN values.')\n",
        "        model.optimizer.apply_gradients(zip(grads, model.trainable_variables))  \n",
        "\n",
        "  train_acc /= 6300*2\n",
        "  print('# epoch loss:  ', epoch_loss.numpy(), '; epoch acc: ', train_acc)\n",
        "  \n",
        "  print(\"------ testing -------\")\n",
        "  if epoch >= 0: # 5\n",
        "    start_test   = datetime.now()\n",
        "    test_acc     = 0\n",
        "    img_test     = 0\n",
        "    for n_batch_test in range(560):\n",
        "      x_test, y_true_test, n_imgs = test_generator.get_batch(n_batch_test)\n",
        "      y_pred_test         = model(x_test)\n",
        "      test_acc           += Accuracy(y_true_test, y_pred_test)\n",
        "      img_test += n_imgs\n",
        "    print(img_test)\n",
        "    test_acc /= 25200\n",
        "    print('# test accuray: ', test_acc, '  and time needed for test: ', datetime.now()-start_test)\n",
        "       # Save model when successfully testing\n",
        "    if (test_acc > old_test_acc): \n",
        "      old_test_acc = test_acc\n",
        "      model.save(best_model_file)\n",
        "      print('Save model completed')\n",
        "      with open(os.path.join(stored_dir,\"train_log.txt\"), \"a\") as text_file:\n",
        "        text_file.write(\"Save best model at Epoch: {}; Accuracy: {}\\n\".format(epoch, old_test_acc))\n",
        "\n",
        "  with open(os.path.join(stored_dir,\"train_log.txt\"), \"a\") as text_file:\n",
        "    text_file.write(\"Epoch: {}; lr: {}; Train accuracy: {}\\n\".format(epoch, model.optimizer.learning_rate.numpy(), train_acc))\n",
        "  print('# epoch training time: ', datetime.now()-s, '\\n')\n"
      ],
      "metadata": {
        "id": "OlGfpOwXtj19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss_1.numpy(), loss_2.numpy(), epoch_loss.numpy(), test_acc )"
      ],
      "metadata": {
        "id": "qQ47nR7stym9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train.py for model_2"
      ],
      "metadata": {
        "id": "Zu_M9jPTtwbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "def Accuracy(y_true, y_pred):\n",
        "  if y_true.shape != y_pred.shape:\n",
        "    print('Error metric !')\n",
        "    raise ValueError('Something error in Acc calculation')\n",
        "  y_true   = tf.argmax(y_true, axis=-1) # (B) \n",
        "  y_pred   = tf.argmax(y_pred, axis=-1) # (B)\n",
        "  correct  = tf.cast(y_true == y_pred, tf.float32)\n",
        "  accuracy = tf.reduce_sum(correct)\n",
        "  return accuracy.numpy()\n",
        "\n",
        "def lr_schedule(epoch, lr):\n",
        "  if epoch < 6:\n",
        "    return 1e-7\n",
        "  elif epoch < 15:\n",
        "    return 6e-8 \n",
        "  elif epoch < 20:\n",
        "    return 2.7e-8\n",
        "  else:\n",
        "    return 1e-8\n",
        "\n",
        "TRAIN_DIR       = '/content/dataset2/train/'\n",
        "TEST_DIR        = '/content/dataset2/test/'\n",
        "stored_dir      = '/content/drive/MyDrive/RSIC/NWPU-RESISC45/effb0_567_con_2loss_2'\n",
        "best_model_file = '/content/drive/MyDrive/RSIC/NWPU-RESISC45/effb0_567_con_2loss_2/contrastive_model.h5'\n",
        "BATCH_SIZE      = 15  # 3 or 5 or 9 or 15 or 45\n",
        "alpha           = 0.6 # tuning parameter\n",
        "current_epoch   = 32\n",
        "old_test_acc    = 0.9383333333333334\n",
        "train_generator = TrainGenerator(img_dir=TRAIN_DIR, batch_size=BATCH_SIZE)\n",
        "test_generator  = TestGenerator(img_dir=TEST_DIR)\n",
        "\n",
        "\n",
        "for epoch in range(current_epoch, 127):\n",
        "  print('\\n\\n ==================== Epoch: ', epoch,'======================')\n",
        "  s = datetime.now()\n",
        "  train_acc = 0\n",
        "  epoch_loss = 0\n",
        "  label_dict = hypara().label_dict\n",
        "  class_list = list(label_dict.keys())\n",
        "  random.shuffle(class_list)\n",
        "\n",
        "  # if epoch == current_epoch:\n",
        "  #   model.optimizer.learning_rate = 1e-6\n",
        "  # else:  \n",
        "  model.optimizer.learning_rate = lr_schedule(epoch, model.optimizer.learning_rate.numpy())\n",
        "  print(' *** learning rate: ', model.optimizer.learning_rate)\n",
        "  print('-------- training ---------')\n",
        "  for i in range(int(45 / BATCH_SIZE)):\n",
        "    for n_batch_train in range(140):\n",
        "      x_train, y_true_train = train_generator.get_batch(idx_num=n_batch_train, class_list=class_list[i*BATCH_SIZE:(i+1)*BATCH_SIZE], is_aug=True) # return 2 batches of images, each batch contain B images from B class\n",
        "      with tf.GradientTape() as tape:\n",
        "        y_pred_train = model(x_train)\n",
        "        train_acc   += Accuracy(y_true_train, y_pred_train)\n",
        "        loss_1       = model.loss[0](y_true_train, y_pred_train)                            # categorical cross entropy\n",
        "        loss_2       = model.loss[1](y_pred_train[:BATCH_SIZE], y_pred_train[BATCH_SIZE:])  # euclid distance + consine similairy\n",
        "        loss         = alpha*loss_1 + (1-alpha)*loss_2                                      # total loss \n",
        "        epoch_loss  += loss \n",
        "        grads        = tape.gradient(loss, model.trainable_variables) \n",
        "        tf.debugging.check_numerics(grads[0], 'grad contains NaN values.')\n",
        "        model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  train_acc /= 6300*2\n",
        "  print('# epoch loss:  ', epoch_loss.numpy(), '; epoch acc: ', train_acc)\n",
        "  \n",
        "  print(\"------ testing -------\")\n",
        "  if epoch >= 0: \n",
        "    start_test   = datetime.now()\n",
        "    test_acc     = 0\n",
        "    img_test     = 0\n",
        "    for n_batch_test in range(560):\n",
        "      x_test, y_true_test, n_imgs = test_generator.get_batch(n_batch_test)\n",
        "      y_pred_test         = model(x_test)\n",
        "      test_acc           += Accuracy(y_true_test, y_pred_test)\n",
        "      img_test += n_imgs\n",
        "    print(img_test)\n",
        "    test_acc /= 25200\n",
        "    print('# test accuray: ', test_acc, '  and time needed for test: ', datetime.now()-start_test)\n",
        "       # Save model when successfully testing\n",
        "    if (test_acc > old_test_acc): \n",
        "      old_test_acc = test_acc\n",
        "      model.save(best_model_file)\n",
        "      print('Save model completed')\n",
        "      with open(os.path.join(stored_dir,\"train_log.txt\"), \"a\") as text_file:\n",
        "        text_file.write(\"Save best model at Epoch: {}; Accuracy: {}\\n\".format(epoch, old_test_acc))\n",
        "\n",
        "  with open(os.path.join(stored_dir,\"train_log.txt\"), \"a\") as text_file:\n",
        "    text_file.write(\"Epoch: {}; lr: {}; Train accuracy: {}\\n\".format(epoch, model.optimizer.learning_rate.numpy(), train_acc))\n",
        "  print('# epoch training time: ', datetime.now()-s, '\\n')\n"
      ],
      "metadata": {
        "id": "cg8HWGxbtvsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss_1.numpy(), loss_2.numpy(),loss.numpy(), epoch_loss.numpy())"
      ],
      "metadata": {
        "id": "l5Jg26wmt-am"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}